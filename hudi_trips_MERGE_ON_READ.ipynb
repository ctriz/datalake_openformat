{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fa63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete this file. It is used to set environment variables for PySpark in Jupyter Notebooks.\n",
    "# Helper script to set PySpark environment variables for Jupyter Notebooks\n",
    "import os\n",
    "\n",
    "# Define a function to set PySpark environment variables\n",
    "def set_pyspark_env_vars():\n",
    "    \"\"\"\n",
    "    Sets the PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON environment variables.\n",
    "    This is necessary to help Spark find the correct Python interpreter.\n",
    "    You MUST replace the path below with the absolute path to your python.exe.\n",
    "    \"\"\"\n",
    "    os.environ['PYSPARK_PYTHON'] = \"path_to_your_python_executable\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = \"path_to_your_python_executable\"\n",
    "    print(\"PySpark environment variables set.\")\n",
    "\n",
    "set_pyspark_env_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a helper function to get the path of the project directory\n",
    "import os\n",
    "def get_notebook_path():\n",
    "        try:\n",
    "        # This works for scripts run from the command line\n",
    "            SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # This is the dynamic way for Jupyter Notebooks\n",
    "            SCRIPT_DIR = os.path.abspath(os.getcwd())\n",
    "        return SCRIPT_DIR\n",
    "    \n",
    "# Get the base path to your Hudi table \n",
    "SCRIPT_DIR = get_notebook_path()\n",
    "HUDI_BASE_PATH = os.path.join(SCRIPT_DIR, \"hudi_copy_on_write_table_data\")\n",
    "print(f\"Notebook path: {SCRIPT_DIR}\")\n",
    "print(f\"Hudi base path: {HUDI_BASE_PATH}\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to create a Spark session with Hudi jars\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    print(\"Creating Spark session with Hudi jars...\")\n",
    "    # Import necessary libraries\n",
    "    \n",
    "    import os\n",
    "    import findspark\n",
    "\n",
    "    # Findspark helps to find the Spark installation on your system\n",
    "    # You might need to add this line if it's not already in your path\n",
    "    findspark.init(spark_home=os.environ.get(\"SPARK_HOME\"))\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Path to the directory containing Hudi jar files\n",
    "    jars_path = os.path.join(SCRIPT_DIR, \"jars\")\n",
    "    print(f\"Resolved jars path: {jars_path}\")\n",
    "\n",
    "    hudi_jars = [\n",
    "            os.path.join(jars_path, 'hudi-spark3.4-bundle_2.12-0.14.1.jar')\n",
    "        ]\n",
    "    print(f\"Hudi jars will be loaded from: {hudi_jars}\")\n",
    "    print(f\"Jars path resolved to: {jars_path}\")\n",
    "    \n",
    "    # Configure Spark session with Hudi configurations and jars\n",
    "    # Adjust Spark configurations as necessary\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HudiExplore-TripApp-MergeOnRead\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "        .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\") \\\n",
    "        .config(\"spark.jars\", \",\".join(hudi_jars)) \\\n",
    "        .getOrCreate()\n",
    "    print(\"Spark session with Hudi jars has been created.\")\n",
    "    return spark\n",
    "create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7297da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create sample data for this example\n",
    "# Adjust the schema and data as necessary for your use case\n",
    "\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def create_sample_data(number_of_records):     # Schema for ride-sharing data\n",
    "\n",
    "    # Generate large dataset (simulate 100 trips)\n",
    "    # The ts field should be a proper datetime object for TimestampType\n",
    "        \n",
    "    data = [(f\"trip_{i}\", datetime.fromtimestamp(time.time() + i), f\"rider_{i% 1000}\", f\"driver_{i%500}\", 20.0 + (i%50), \n",
    "            [\"nyc\", \"sf\", \"la\"][i%3], f\"2025-09-{(i%30)+1:02}\") for i in range(number_of_records)]\n",
    "    # print data --- IGNORE ---\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "#create_sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create Spark dataframe from sample data\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_dataframe(spark, data):\n",
    "    # Schema for ride-sharing data\n",
    "    schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider_id\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True)  # For partitioning\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    # Convert trip_date string to actual date type\n",
    "    # For partitioning, Hudi expects the partition field to be of date type if it's a date\n",
    "    df = df.withColumn(\"trip_date\", to_date(df[\"trip_date\"], \"yyyy-MM-dd\")) \n",
    "    print(\"Sample DataFrame created:\")\n",
    "    df.show(5)\n",
    "    return df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "#create_dataframe(spark, create_sample_data(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38839018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to delete existing Hudi table directory if it exists\n",
    "# This ensures a clean state for the example\n",
    "import shutil\n",
    "def cleanup_hudi_table():\n",
    "    try:\n",
    "        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "        HUDI_TABLE_NAME = \"hudi_MERGE_ON_READ\"\n",
    "        HUDI_BASE_PATH = os.path.join(SCRIPT_DIR, \"hudi_mor_table\")\n",
    "        if os.path.exists(HUDI_BASE_PATH):\n",
    "            print(f\"Cleaning up existing data at {HUDI_BASE_PATH}\")\n",
    "            shutil.rmtree(HUDI_BASE_PATH)\n",
    "        else:\n",
    "            print(f\"Directory {HUDI_BASE_PATH} does not exist, no cleanup needed.\")\n",
    "    except NameError:\n",
    "        SCRIPT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "#cleanup_hudi_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get Hudi parameters\n",
    "import os\n",
    "\n",
    "\n",
    "def get_hudi_param():\n",
    "    print(\"Initializing Hudi parameters...\")\n",
    "   \n",
    "    try:\n",
    "        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # Fallback to the current working directory if __file__ is not defined\n",
    "        SCRIPT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "    # Define the Hudi table name and base path\n",
    "    HUDI_TABLE_NAME = \"hudi_trips_table_mor\"\n",
    "    HUDI_BASE_PATH = os.path.join(SCRIPT_DIR, \"hudi_table_data_mor\")\n",
    "\n",
    "    # Define Hudi parameters\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': HUDI_TABLE_NAME,\n",
    "        'hoodie.datasource.write.recordkey.field': \"trip_id\",\n",
    "        'hoodie.datasource.write.partitionpath.field': \"city\",\n",
    "        'hoodie.datasource.write.precombine.field': \"ts\",\n",
    "        'hoodie.datasource.write.operation': 'upsert',\n",
    "        'hoodie.upsert.shuffle.parallelism': 2,\n",
    "        'hoodie.insert.shuffle.parallelism': 2\n",
    "    }\n",
    "\n",
    "    print(f\"Hudi parameters initialized: {hudi_options}\")\n",
    "    return hudi_options, HUDI_BASE_PATH\n",
    "\n",
    "# Call this function\n",
    "get_hudi_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform the initial bulk insert to Hudi table with Merge-On-Read storage\n",
    "# Note: The table type can be either COPY_ON_WRITE or MERGE_ON_READ. Here we use MERGE_ON_READ for simplicity.\n",
    "import sys\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def bulk_insert_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing bulk insert to Hudi table...\")\n",
    "    # Create a DataFrame with new data for bulk insert\n",
    "    bulk_data = create_sample_data(50)  # Create 50 new sample records\n",
    "    bulk_df = create_dataframe(spark, bulk_data)\n",
    "    # Map the generated columns to the Hudi schema\n",
    "    bulk_df = bulk_df.withColumn(\"uuid\", monotonically_increasing_id().cast(StringType()))\n",
    "    bulk_df = bulk_df.withColumnRenamed(\"rider_id\", \"rider\")\n",
    "\n",
    "    # Set the table type to Copy-on-Write for the bulk insert\n",
    "    hudi_options_bulk = hudi_options_base.copy()\n",
    "    hudi_options_bulk['hoodie.datasource.write.table.type'] = 'MERGE_ON_READ'\n",
    "    hudi_options_bulk['hoodie.datasource.write.operation'] = 'bulk_insert'\n",
    "    \n",
    "    print(\"Writing bulk inserted data to Hudi table...\")\n",
    "    bulk_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_bulk) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table bulk inserted successfully.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)  \n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the bulk insert function\n",
    "# Perform the bulk insert\n",
    "bulk_insert_hudi(spark, hudi_options_base, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f364a",
   "metadata": {},
   "source": [
    "The Hudi table on disk carries the schema from its first creation. When you perform an upsert with new data, Hudi performs the following steps:\n",
    "\n",
    "Reads the Existing Schema: It checks the schema of the Hudi table already saved on disk.\n",
    "\n",
    "Aligns the New Data: It compares the schema of your new upsert_df with the existing table schema.\n",
    "\n",
    "Applies Schema Evolution: If the data does not contain a specific column, Hudi correctly recognizes this and automatically adds the  column to the incoming data with null values to match the existing table schema.\n",
    "\n",
    "This Schema Evolution behavior is by design. It allows to add or drop columns over time without having to rebuild the entire table, making schema changes much more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to do incremental updates to Hudi table with Merge-On-Read storage in Upsert mode\n",
    "\n",
    "\n",
    "\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def upsert_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing upsert update to Hudi table...\")\n",
    "    # Create a DataFrame with a new data and one updated data\n",
    "    upsert_data = [\n",
    "        (\"trip_2\", datetime.fromtimestamp(time.time() + 101), \"rider_2_updated\", \"driver_2\", 100.0, \"la\", \"2025-09-03\"),\n",
    "        (\"trip_101\", datetime.fromtimestamp(time.time() + 102), \"rider_101\", \"driver_101\", 200.0, \"nyc\", \"2025-09-01\")\n",
    "    ]\n",
    "    upsert_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True)\n",
    "    ])\n",
    "    upsert_df = spark.createDataFrame(upsert_data, upsert_schema)\n",
    "    upsert_df = upsert_df.withColumn(\"uuid\", lit(None).cast(StringType())) # We'll let Hudi determine the uuid for new records\n",
    "    \n",
    "    # Set the table type to Copy-on-Write for the upsert\n",
    "    hudi_options_upsert = hudi_options_base.copy()\n",
    "    hudi_options_upsert['hoodie.datasource.write.table.type'] = 'COPY_ON_WRITE'\n",
    "    hudi_options_upsert['hoodie.datasource.write.operation'] = 'upsert'\n",
    "    print(\"Writing upserted data to Hudi table...\")\n",
    "    upsert_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_upsert) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table upserted successfully.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "\n",
    "# Call the upsert update function\n",
    "upsert_hudi(spark, hudi_options_base, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba8e3a",
   "metadata": {},
   "source": [
    "Now, that the initial bulk load is done using Copy On Write storage, lets get to the heart of the Apache Hudi. \n",
    "The distinction between snapshot and read-optimized queries is crucial, especially when deciding how to access the upsert data.\n",
    "\n",
    "Snapshot Query\n",
    "A snapshot query provides the most up-to-date view of a Hudi table. It reads the latest committed state of the data, ensuring you get a complete and accurate \"snapshot\" of the table at a specific point in time.\n",
    "\n",
    "How it works (for a Copy on Write table): With a Copy on Write (CoW) table, every update or insert operation creates a new version of the Parquet base file. A snapshot query simply reads all the latest versions of these Parquet files. There are no incremental files to merge, so the query is straightforward and efficient.\n",
    "\n",
    "When to choose it: This is the default and recommended choice for most use cases with CoW tables. It's the simplest way to read the current state of your data. The code spark.read.format(\"hudi\").load(HUDI_BASE_PATH) performs a snapshot query by default.\n",
    "\n",
    "Read-Optimized Query\n",
    "A read-optimized query is designed to be as fast as possible by reading only the Parquet base files, completely ignoring any recent incremental data.\n",
    "\n",
    "How it works (for a Copy on Write table): Since a CoW table already stores all its data in Parquet base files, a read-optimized query behaves exactly the same as a snapshot query. It reads the same set of files and provides the identical result. You will not see a performance difference between the two for a CoW table.\n",
    "\n",
    "When to choose it: This query type is primarily useful for Merge on Read (MoR) tables, not Copy on Write. With a MoR table, a read-optimized query would be faster than a snapshot query because it skips the step of merging the Parquet base files with the Avro/ORC delta logs.  However, this speed comes at the cost of having potentially stale data, as it won't include recent updates that are only in the delta logs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will do a snapshot query on the Hudi table and display the results.\n",
    "# In a Merge-On-Read (MoR) table, data is stored in a combination of columnar base files and row-based log files.\n",
    "# When performing a snapshot query, Hudi automatically merges the base files with the log files to present the latest view of the data.\n",
    "# The merge happens on the fly to present with the most up-to-date data.\n",
    "# This ensures full data freshness but can be slower due to the merging overhead.\n",
    "# This means you get a fast, efficient query that's ideal for a read-heavy workload.\n",
    "# Merge-On-Read (MoR) tables are designed to handle high write throughput and provide efficient storage for large datasets.\n",
    "# The downside is that the results may not include the most recent data because the latest updates and inserts are still sitting in the log files.\n",
    "\n",
    "\n",
    "def snapshot_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing snapshot query on Hudi table...\")\n",
    "    trip_snapshot_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    print(\"Snapshot DataFrame loaded from Hudi table:\")\n",
    "    trip_snapshot_df.show(5)\n",
    "    print(\"Creating temporary view for SQL queries...\")\n",
    "    trip_snapshot_df.createOrReplaceTempView(\"hudi_trips_snapshot\")\n",
    "    print(\"Snapshot query result:\")\n",
    "    spark.sql(\"SELECT * FROM hudi_trips_snapshot ORDER BY ts DESC\").show(10)\n",
    "    spark.sql(\"SELECT city, COUNT(*) as trip_count FROM hudi_trips_snapshot GROUP BY city ORDER BY trip_count DESC\").show()\n",
    "    # Saving this to a df for further use if needed\n",
    "    hudi_snapshot_df = spark.sql(\"SELECT * FROM hudi_trips_snapshot ORDER BY ts DESC\")\n",
    "    hudi_snapshot_df.show(5)\n",
    "    return trip_snapshot_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the snapshot query function\n",
    "snapshot_query_hudi(spark, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f988b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funtion will do a read-optimized query on the Hudi table to show the latest state of the data\n",
    "# In a Merge-On-Read (MoR) table, data is stored in a combination of columnar base files and row-based log files.\n",
    "# When performing a read-optimized query, Hudi reads only the columnar base files without merging in the log files.\n",
    "# This makes read-optimized queries very fast and efficient, as they can leverage the optimized columnar storage format.\n",
    "# The output is exactly the same as the default spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "# This makes read-optimized queries very fast and efficient, as they can leverage the optimized columnar storage format.\n",
    "# The downside is that the results may not include the most recent data because the latest updates and inserts are still sitting in the log files.  \n",
    "\n",
    "\n",
    "def read_optimized_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing read-optimized query on Hudi table...\")\n",
    "    trip_ro_df = spark.read.format(\"hudi\").option(\"hoodie.datasource.query.type\", \"read_optimized\").load(HUDI_BASE_PATH)\n",
    "    print(\"Read-Optimized DataFrame loaded from Hudi table:\")\n",
    "    trip_ro_df.show(5)\n",
    "    print(\"Creating temporary view for SQL queries...\")\n",
    "    trip_ro_df.createOrReplaceTempView(\"hudi_trips_ro\")\n",
    "    print(\"Read-Optimized query result:\")\n",
    "    spark.sql(\"SELECT * FROM hudi_trips_ro ORDER BY ts DESC\").show(10)\n",
    "    spark.sql(\"SELECT city, COUNT(*) as trip_count FROM hudi_trips_ro GROUP BY city ORDER BY trip_count DESC\").show()\n",
    "    # Saving this to a df for further use if needed\n",
    "    hudi_ro_df = spark.sql(\"SELECT * FROM hudi_trips_ro ORDER BY ts DESC\")\n",
    "    hudi_ro_df.show(5)\n",
    "    return trip_ro_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the read optimized function\n",
    "read_optimized_query_hudi(spark,hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75894725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create new commits and records in the Hudi table\n",
    "\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, col, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType, BooleanType\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def upsert_schema_eval(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    \n",
    "    #print existing table schema\n",
    "    print(\"Existing Hudi table schema:\")\n",
    "    spark.read.format(\"hudi\").load(HUDI_BASE_PATH).printSchema()\n",
    "    print(\"Creating new records\")\n",
    "    \n",
    "    #upsert_df=create_dataframe(spark, create_sample_data(5))\n",
    "    # Add a new column 'trip_type' with values \"Personal\" or \"Business\" the for new records\n",
    "    #upsert_df = upsert_df.withColumn(\"trip_type\", lit(\"Personal\"))\n",
    "    # Add a new column 'payment_method' with values \"Credit Card\", \"Cash\", or \"Digital Wallet\" the for new records\n",
    "    #upsert_df = upsert_df.withColumn(\"payment_method\", lit(\"Credit Card\"))\n",
    "    # Add a new column 'is_peak_hour' with boolean values indicating if the trip was during peak hours (e.g., 7-9 AM, 5-7 PM)\n",
    "    #upsert_df = upsert_df.withColumn(\"is_peak_hour\", lit(True)) \n",
    "    # Add a new column 'trip_duration' with random integer values representing the trip duration in minutes\n",
    "    #upsert_df = upsert_df.withColumn(\"trip_duration\", lit(15))\n",
    "    # Add a new column 'trip_distance' with random float values representing the trip distance in miles\n",
    "    #upsert_df = upsert_df.withColumn(\"trip_distance\", lit(5.5))\n",
    "    # Add a new column 'payment_status' with values \"Paid\", \"Pending\", or \"Failed\"\n",
    "    #upsert_df = upsert_df.withColumn(\"payment_status\", lit(\"Paid\"))\n",
    "\n",
    "\n",
    "  # Create new records with the additional columns\n",
    "    new_records = [\n",
    "        (\"trip_100\", datetime.fromtimestamp(time.time()), \"rider_100\", \"driver_100\", 25.0, \"la\", \"2025-09-03\", \"Personal\", \"Credit Card\", True, 15, 5.5, \"Paid\"),\n",
    "        (\"trip_101\", datetime.fromtimestamp(time.time()), \"rider_101\", \"driver_101\", 30.0, \"sf\", \"2025-09-04\", \"Business\", \"Digital Wallet\", False, 30, 10.0, \"Pending\")\n",
    "    ]\n",
    "    new_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True),\n",
    "        StructField(\"trip_type\", StringType(), True),\n",
    "        StructField(\"payment_method\", StringType(), True),\n",
    "        StructField(\"is_peak_hour\", BooleanType(), True),\n",
    "        StructField(\"trip_duration\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"payment_status\", StringType(), True)\n",
    "    ])\n",
    "    # Create a single DataFrame from all new records\n",
    "    upsert_df = spark.createDataFrame(new_records, new_schema)\n",
    "    \n",
    "    # Cast the trip_date column to the correct DateType to match the existing table schema\n",
    "    upsert_df = upsert_df.withColumn(\"trip_date\", to_date(col(\"trip_date\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Add the uuid column as required by Hudi\n",
    "    upsert_df = upsert_df.withColumn(\"uuid\", lit(None).cast(StringType()))\n",
    "    \n",
    "    print(\"New records with additional columns:\")\n",
    "    upsert_df.show()\n",
    "    print(\"New schema with additional columns:\")\n",
    "    upsert_df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "    # Set the table type to Merge-on-Read and enable schema evolution\n",
    "    hudi_options_upsert = hudi_options_base.copy()\n",
    "    hudi_options_upsert['hoodie.datasource.write.table.type'] = 'MERGE_ON_READ'\n",
    "    hudi_options_upsert['hoodie.datasource.write.operation'] = 'upsert'\n",
    "    hudi_options_upsert['hoodie.datasource.write.schema.allow.auto.evolution'] = 'true'\n",
    "    hudi_options_upsert['hoodie.datasource.write.schema.on-read.enable'] = 'true'\n",
    "\n",
    "    print(\"Writing upserted data to Hudi table...\")\n",
    "    upsert_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_upsert) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table upserted successfully.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "    \n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the batch function\n",
    "upsert_schema_eval(spark, hudi_options_base, hudi_base_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8096f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def incremental_query_hudi(spark, hudi_base_path):\n",
    "    \"\"\"\n",
    "    Performs an incremental query on the Hudi table to fetch only the new data.\n",
    "    \n",
    "    This function reads the latest commit instant time from the Hudi timeline\n",
    "    and uses the second-to-last commit as the starting point for the incremental query.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Reading table for latest commit time for incremental query...\")\n",
    "    # Get the latest commit time from the Hudi table\n",
    "    commits = list(map(lambda row: row[0],\n",
    "                       spark.read.format(\"hudi\")\n",
    "                       .load(hudi_base_path)\n",
    "                       .select(\"_hoodie_commit_time\")\n",
    "                       .distinct()\n",
    "                       .orderBy(\"_hoodie_commit_time\")\n",
    "                       .collect()))\n",
    "    \n",
    "    if not commits:\n",
    "        print(\"No commits found. Cannot perform incremental query.\")\n",
    "        return\n",
    "    if len(commits) < 2:\n",
    "        print(\"Not enough commits to perform incremental query from second-to-last commit.\")\n",
    "        print(f\"Commits found: {commits}\")\n",
    "        return\n",
    "    #latest_commit = commits[len(commits) - 1]\n",
    "    second_to_last_commit = commits[len(commits) - 2]\n",
    "    print(f\"Second-to-last commit instant: {second_to_last_commit}\")\n",
    "    \n",
    "    \n",
    "    print(f\"Latest commit time for incremental query: {second_to_last_commit}\")\n",
    "    \n",
    "    # Perform the incremental query\n",
    "    incremental_query_options = {\n",
    "        'hoodie.datasource.query.type': 'incremental',\n",
    "        'hoodie.datasource.query.incremental.pull_style': 'full_scan',\n",
    "        'hoodie.datasource.read.begin.instanttime': second_to_last_commit\n",
    "    }\n",
    "\n",
    "    print(\"Executing incremental query on Hudi table...\")\n",
    "    incremental_df = spark.read.format(\"hudi\") \\\n",
    "        .options(**incremental_query_options) \\\n",
    "        .load(hudi_base_path)\n",
    "\n",
    "    print(\"Incremental query results:\")\n",
    "    incremental_df.show()\n",
    "    \n",
    "    return incremental_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the incremental query function\n",
    "incremental_query_hudi(spark, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to hard delete records from Hudi table\n",
    "def hard_delete_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing hard delete on Hudi table...\")\n",
    "    # Create a DataFrame with the record keys to delete\n",
    "    delete_data = [\n",
    "        (\"trip_3\",),  # Assuming trip_3 exists\n",
    "        (\"trip_50\",)  # Assuming trip_50 exists\n",
    "    ]\n",
    "    delete_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False)\n",
    "    ])\n",
    "    delete_df = spark.createDataFrame(delete_data, delete_schema)\n",
    "    \n",
    "    # Set the operation to 'delete'\n",
    "    hudi_options_delete = hudi_options_base.copy()\n",
    "    hudi_options_delete['hoodie.datasource.write.operation'] = 'delete'\n",
    "    \n",
    "    print(\"Writing delete records to Hudi table...\")\n",
    "    delete_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_delete) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table hard delete completed.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the hard delete function\n",
    "hard_delete_hudi(spark, hudi_options_base, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df263ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do a new indexed read query on the Hudi table\n",
    "def indexed_read_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing indexed read query on Hudi table...\")\n",
    "    trip_indexed_df = spark.read.format(\"hudi\").option(\"hoodie.datasource.query.type\", \"snapshot\").load(HUDI_BASE_PATH)\n",
    "    print(\"Indexed Read DataFrame loaded from Hudi table:\")\n",
    "    trip_indexed_df.show(5)\n",
    "    print(\"Creating temporary view for SQL queries...\")\n",
    "    trip_indexed_df.createOrReplaceTempView(\"hudi_trips_indexed\")\n",
    "    print(\"Indexed read query result:\")\n",
    "    spark.sql(\"SELECT * FROM hudi_trips_indexed ORDER BY ts DESC\").show(10)\n",
    "    spark.sql(\"SELECT city, COUNT(*) as trip_count FROM hudi_trips_indexed GROUP BY city ORDER BY trip_count DESC\").show()\n",
    "    # Saving this to a df for further use if needed\n",
    "    hudi_indexed_df = spark.sql(\"SELECT * FROM hudi_trips_indexed ORDER BY ts DESC\")\n",
    "    hudi_indexed_df.show(5)\n",
    "    return trip_indexed_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the indexed read query function\n",
    "indexed_read_query_hudi(spark, hudi_base_path)\n",
    "# This function will clean up the Hudi table data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to show time travel query on the Hudi table\n",
    "def time_travel_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing time travel query on Hudi table...\")\n",
    "    \n",
    "    # Read all commits and find the earliest one\n",
    "    # Note: This loads metadata, not the full dataset, so it's efficient.\n",
    "    try:\n",
    "        commits = spark.read.format(\"hudi\").load(HUDI_BASE_PATH).select(\"_hoodie_commit_time\").distinct().collect()\n",
    "        if not commits:\n",
    "            print(\"No commits found in the table. Cannot perform time travel query.\")\n",
    "            return\n",
    "        \n",
    "        # Sort commits and get the earliest one\n",
    "        commit_times = sorted([row[\"_hoodie_commit_time\"] for row in commits])\n",
    "        print(f\"Available commits: {commit_times}\")\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"Error reading commits. Make sure the Hudi table exists at: {HUDI_BASE_PATH}\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # For demonstration, we'll pick the second commit if available\n",
    "    if len(commit_times) < 2:\n",
    "        print(\"Not enough commits to perform time travel. Need at least 2 commits.\")\n",
    "        return\n",
    "    \n",
    "    travel_time = commit_times[1]\n",
    "    print(f\"Time travel to commit: {travel_time}\")\n",
    "\n",
    "    # Perform the time travel query using the selected commit time\n",
    "    time_travel_df = spark.read.format(\"hudi\") \\\n",
    "        .option(\"as.of.instant\", travel_time) \\\n",
    "        .load(HUDI_BASE_PATH)\n",
    "    print(\"Time travel query DataFrame loaded.\")\n",
    "    time_travel_df.show(5)\n",
    "    # Run a sample SQL query on the time-traveled data\n",
    "    time_travel_df.createOrReplaceTempView(\"hudi_trips_time_travel\")\n",
    "    print(\"Time travel query result:\")\n",
    "    spark.sql(\"SELECT trip_id, fare, ts FROM hudi_trips_time_travel ORDER BY ts DESC\").show(10)\n",
    "    print(\"Time travel query completed.\")\n",
    "    return time_travel_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the time travel query function\n",
    "time_travel_query_hudi(spark, hudi_base_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
