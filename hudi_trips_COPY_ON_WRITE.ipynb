{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World from Jupyter!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fa63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete this file. It is used to set environment variables for PySpark in Jupyter Notebooks.\n",
    "# Helper script to set PySpark environment variables for Jupyter Notebooks\n",
    "import os\n",
    "\n",
    "# Define a function to set PySpark environment variables\n",
    "def set_pyspark_env_vars():\n",
    "    \"\"\"\n",
    "    Sets the PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON environment variables.\n",
    "    This is necessary to help Spark find the correct Python interpreter.\n",
    "    You MUST replace the path below with the absolute path to your python.exe.\n",
    "    \"\"\"\n",
    "    os.environ['PYSPARK_PYTHON'] = \"path_to_your_python_executable\"\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = \"path_to_your_python_executable\"\n",
    "    print(\"PySpark environment variables set.\")\n",
    "\n",
    "set_pyspark_env_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a helper function to get the path of the project directory\n",
    "import os\n",
    "def get_notebook_path():\n",
    "        try:\n",
    "        # This works for scripts run from the command line\n",
    "            SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # This is the dynamic way for Jupyter Notebooks\n",
    "            SCRIPT_DIR = os.path.abspath(os.getcwd())\n",
    "        return SCRIPT_DIR\n",
    "    \n",
    "# Get the base path to your Hudi table \n",
    "SCRIPT_DIR = get_notebook_path()\n",
    "HUDI_BASE_PATH = os.path.join(SCRIPT_DIR, \"hudi_copy_on_write_table_data\")\n",
    "print(f\"Notebook path: {SCRIPT_DIR}\")\n",
    "print(f\"Hudi base path: {HUDI_BASE_PATH}\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to create a Spark session with Hudi jars\n",
    "# This code assumes you have Apache Spark installed and configured on your system.\n",
    "# It also assumes you have Java installed, as Spark requires Java to run.\n",
    "# Make sure to set the SPARK_HOME environment variable to point to your Spark installation directory.\n",
    "# For example, on Windows, you might set it like this:\n",
    "# os.environ['SPARK_HOME'] = \"C:\\\\path\\\\to\\\\your\\\\spark\"\n",
    "# You also need to add the Spark bin directory to your PATH environment variable.\n",
    "# For example:\n",
    "# os.environ['PATH'] += os.pathsep + os.path.join(os.environ['SPARK_HOME'], 'bin')\n",
    "# You need to have the Hudi jar files downloaded and placed in a known directory.\n",
    "# You can download the Hudi jars from the Apache Hudi releases page:\n",
    "# https://hudi.apache.org/releases.html\n",
    "# Make sure to download the correct version that matches your Spark and Scala versions.\n",
    "# For example, for Spark 3.4 and Scala 2.12, you would download the hudi-spark3.4-bundle_2.12-0.14.1.jar\n",
    "# Place the jar files in a directory, e.g., \"jars\" in the same directory as this notebook.\n",
    "# Adjust the path in the code below to point to your jar files.\n",
    "# Define a function to create a Spark session with Hudi jars\n",
    "\n",
    "def create_spark_session():\n",
    "    print(\"Creating Spark session with Hudi jars...\")\n",
    "    # Import necessary libraries\n",
    "    \n",
    "    import os\n",
    "    import findspark\n",
    "\n",
    "    # Findspark helps to find the Spark installation on your system\n",
    "    # You might need to add this line if it's not already in your path\n",
    "    findspark.init(spark_home=os.environ.get(\"SPARK_HOME\"))\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Path to the directory containing Hudi jar files\n",
    "    jars_path = os.path.join(SCRIPT_DIR, \"jars\")\n",
    "    print(f\"Resolved jars path: {jars_path}\")\n",
    "\n",
    "    hudi_jars = [\n",
    "            os.path.join(jars_path, 'hudi-spark3.4-bundle_2.12-0.14.1.jar')\n",
    "        ]\n",
    "    print(f\"Hudi jars will be loaded from: {hudi_jars}\")\n",
    "    print(f\"Jars path resolved to: {jars_path}\")\n",
    "    \n",
    "    # Configure Spark session with Hudi configurations and jars\n",
    "    # Adjust Spark configurations as necessary\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HudiExplore-TripApp-CopyonWrite\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "        .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\") \\\n",
    "        .config(\"spark.jars\", \",\".join(hudi_jars)) \\\n",
    "        .getOrCreate()\n",
    "    print(\"Spark session with Hudi jars has been created.\")\n",
    "    return spark\n",
    "create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code verifies that the Spark session and Hudi dependencies are working\n",
    "import os, shutil, sys\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Ensure PySpark environment variables are set\n",
    "set_pyspark_env_vars()\n",
    "\n",
    "# This function verifies that the Spark session and Hudi dependencies are working\n",
    "def verify_hudi_setup():\n",
    "    \"\"\"\n",
    "    Verifies the Hudi setup by writing to and reading from a Hudi table.\n",
    "    This function assumes a SparkSession has already been created with Hudi jars.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the existing SparkSession from the notebook\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark is None:\n",
    "            print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "            create_spark_session()\n",
    "            spark = SparkSession.getActiveSession()\n",
    "            if spark is None:\n",
    "                raise Exception(\"Failed to create SparkSession.\")\n",
    "        else:\n",
    "            print(\"Calling SparkSession.getActiveSession() succeeded.\")\n",
    "        print(\"Verification: SparkSession is active.\")\n",
    "                   \n",
    "        # Define a temporary path for the test Hudi table.\n",
    "        # os.path.abspath() converts the relative path to an absolute path,\n",
    "        # which is required by Hudi's Java-based file system layer on Windows.\n",
    "        temp_hudi_path = os.path.abspath(\"./temp_hudi_test\")\n",
    "        \n",
    "        # Clean up any previous test data\n",
    "        if os.path.exists(temp_hudi_path):\n",
    "            print(f\"Cleaning up previous test data at {temp_hudi_path}\")\n",
    "            shutil.rmtree(temp_hudi_path)\n",
    "\n",
    "        # Create a simple DataFrame to write to the Hudi table\n",
    "        print(\"Verification: Creating a simple DataFrame...\")\n",
    "        test_data = [\n",
    "            (\"1\", \"item_A\", \"2023-01-01\", \"1672531200\"),\n",
    "            (\"2\", \"item_B\", \"2023-01-01\", \"1672531200\")\n",
    "        ]\n",
    "        test_columns = [\"uuid\", \"item_name\", \"partitionpath\", \"ts\"]\n",
    "        test_df = spark.createDataFrame(test_data, test_columns)\n",
    "        \n",
    "        # Hudi options for the write operation\n",
    "        hudi_options = {\n",
    "            'hoodie.table.name': 'temp_test_table',\n",
    "            'hoodie.datasource.write.precombine.field': 'ts',\n",
    "            'hoodie.datasource.write.recordkey': 'uuid',\n",
    "            'hoodie.datasource.write.partitionpath.field': 'partitionpath'\n",
    "        }\n",
    "\n",
    "        # Write the DataFrame to the Hudi table\n",
    "        print(\"Verification: Writing data to Hudi table...\")\n",
    "        test_df.write.format(\"hudi\") \\\n",
    "            .options(**hudi_options) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(temp_hudi_path)\n",
    "            \n",
    "        print(\"Verification: Data successfully written to Hudi.\")\n",
    "\n",
    "        # Read the data back from the Hudi table to verify\n",
    "        print(\"Verification: Reading data from Hudi table...\")\n",
    "        hudi_read_df = spark.read.format(\"hudi\").load(temp_hudi_path)\n",
    "        \n",
    "        print(\"Read data:\")\n",
    "        hudi_read_df.show()\n",
    "        \n",
    "        # Clean up the test data\n",
    "        print(f\"Cleaning up temporary data at {temp_hudi_path}\")\n",
    "        shutil.rmtree(temp_hudi_path)\n",
    "\n",
    "        print(\"Verification Complete: Hudi setup is working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Verification Failed. An error occurred: {e}\")\n",
    "        # You can add more specific error handling here if needed\n",
    "        \n",
    "# Call the verification function\n",
    "verify_hudi_setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c71709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create Spark dataframe from sample data\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "def create_dataframe(spark, data):\n",
    "    # Schema for ride-sharing data\n",
    "    schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider_id\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True)  # For partitioning\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    # Convert trip_date string to actual date type\n",
    "    # For partitioning, Hudi expects the partition field to be of date type if it's a date\n",
    "    df = df.withColumn(\"trip_date\", to_date(df[\"trip_date\"], \"yyyy-MM-dd\")) \n",
    "    print(\"Sample DataFrame created:\")\n",
    "    df.show(5)\n",
    "    return df\n",
    "\n",
    "# Call the function to create Spark dataframe\n",
    "\n",
    "create_spark_session()\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    create_spark_session()\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise Exception(\"Failed to create SparkSession.\")\n",
    "\n",
    "#create_dataframe(spark, create_sample_data(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create sample data for this example\n",
    "# Adjust the schema and data as necessary for your use case\n",
    "\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def create_sample_data(number_of_records):     # Schema for ride-sharing data\n",
    "\n",
    "    # Generate large dataset (simulate 100 trips)\n",
    "    # The ts field should be a proper datetime object for TimestampType\n",
    "        \n",
    "    data = [(f\"trip_{i}\", datetime.fromtimestamp(time.time() + i), f\"rider_{i% 1000}\", f\"driver_{i%500}\", 20.0 + (i%50), \n",
    "            [\"nyc\", \"sf\", \"la\"][i%3], f\"2025-09-{(i%30)+1:02}\") for i in range(number_of_records)]\n",
    "    # print data --- IGNORE ---\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "#create_sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff364a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any previous Hudi table data\n",
    "def cleanup_hudi_table():\n",
    "    try:\n",
    "        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "        HUDI_TABLE_NAME = \"hudi_trips_table\"\n",
    "        HUDI_BASE_PATH = os.path.join(SCRIPT_DIR, \"hudi_table_data\")\n",
    "        if os.path.exists(HUDI_BASE_PATH):\n",
    "            print(f\"Cleaning up existing data at {HUDI_BASE_PATH}\")\n",
    "            shutil.rmtree(HUDI_BASE_PATH)\n",
    "        else:\n",
    "            print(f\"Directory {HUDI_BASE_PATH} does not exist, no cleanup needed.\")\n",
    "    except NameError:\n",
    "        SCRIPT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "#cleanup_hudi_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get Hudi parameters\n",
    "# This function assumes a SparkSession has already been created with Hudi jars.\n",
    "\n",
    "\n",
    "def get_hudi_param():\n",
    "    print(\"Initializing Hudi parameters...\")\n",
    "   \n",
    "    try:\n",
    "        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # Fallback to the current working directory if __file__ is not defined\n",
    "        SCRIPT_DIR = os.path.abspath(os.getcwd())\n",
    "\n",
    "    # Define the Hudi table name and base path\n",
    "    HUDI_TABLE_NAME = \"hudi_trips_table_cow\"\n",
    "    HUDI_BASE_PATH = os.path.join(SCRIPT_DIR, \"hudi_table_data_cow\")\n",
    "\n",
    "    # Define Hudi parameters\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': HUDI_TABLE_NAME,\n",
    "        'hoodie.datasource.write.recordkey.field': \"trip_id\",\n",
    "        'hoodie.datasource.write.partitionpath.field': \"city\",\n",
    "        'hoodie.datasource.write.precombine.field': \"ts\",\n",
    "        'hoodie.datasource.write.operation': 'upsert',\n",
    "        'hoodie.upsert.shuffle.parallelism': 2,\n",
    "        'hoodie.insert.shuffle.parallelism': 2\n",
    "    }\n",
    "\n",
    "    print(f\"Hudi parameters initialized: {hudi_options}\")\n",
    "    return hudi_options, HUDI_BASE_PATH\n",
    "\n",
    "# Call this function\n",
    "get_hudi_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform the initial bulk insert to Hudi table with Copy-on-Write storage\n",
    "# Note: The table type can be either COPY_ON_WRITE \n",
    "import sys\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def bulk_insert_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing bulk insert to Hudi table...\")\n",
    "    # Create a DataFrame with new data for bulk insert\n",
    "    bulk_data = create_sample_data(50)  # Create 50 new sample records\n",
    "    bulk_df = create_dataframe(spark, bulk_data)\n",
    "    # Map the generated columns to the Hudi schema\n",
    "    bulk_df = bulk_df.withColumn(\"uuid\", monotonically_increasing_id().cast(StringType()))\n",
    "    bulk_df = bulk_df.withColumnRenamed(\"rider_id\", \"rider\")\n",
    "\n",
    "    # Set the table type to Copy-on-Write for the bulk insert\n",
    "    hudi_options_bulk = hudi_options_base.copy()\n",
    "    hudi_options_bulk['hoodie.datasource.write.table.type'] = 'COPY_ON_WRITE'\n",
    "    hudi_options_bulk['hoodie.datasource.write.operation'] = 'bulk_insert'\n",
    "    \n",
    "    print(\"Writing bulk inserted data to Hudi table...\")\n",
    "    bulk_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_bulk) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table bulk inserted successfully.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)  \n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the bulk insert function\n",
    "# Perform the bulk insert\n",
    "bulk_insert_hudi(spark, hudi_options_base, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f364a",
   "metadata": {},
   "source": [
    "The Hudi table on disk carries the schema from its first creation. When you perform an upsert with new data, Hudi performs the following steps:\n",
    "\n",
    "Reads the Existing Schema: It checks the schema of the Hudi table already saved on disk.\n",
    "\n",
    "Aligns the New Data: It compares the schema of your new upsert_df with the existing table schema.\n",
    "\n",
    "Applies Schema Evolution: If the data does not contain a specific column, Hudi correctly recognizes this and automatically adds the  column to the incoming data with null values to match the existing table schema.\n",
    "\n",
    "This Schema Evolution behavior is by design. It allows to add or drop columns over time without having to rebuild the entire table, making schema changes much more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to do incremental updates to Hudi table with Copy-on-Write storage in Upsert mode\n",
    "# Note The available options for hoodie.datasource.write.operation are:\n",
    "    #upsert: Inserts new records and updates existing ones.\n",
    "    #insert: Only inserts new records.\n",
    "    #bulk_insert: A more efficient way to insert a large volume of new data.\n",
    "    #delete: Deletes records based on the record key.\n",
    "# \"append\" is a Spark DataFrameWriter mode, which simply tells Spark to add new data without overwriting the table.\n",
    "# When performing an incremental update that includes both new and updated records, the correct Hudi operation to use is upsert.\n",
    "\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def upsert_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing upsert update to Hudi table...\")\n",
    "    # Create a DataFrame with a new data and one updated data\n",
    "    upsert_data = [\n",
    "        (\"trip_2\", datetime.fromtimestamp(time.time() + 101), \"rider_2_updated\", \"driver_2\", 100.0, \"la\", \"2025-09-03\"),\n",
    "        (\"trip_101\", datetime.fromtimestamp(time.time() + 102), \"rider_101\", \"driver_101\", 200.0, \"nyc\", \"2025-09-01\")\n",
    "    ]\n",
    "    upsert_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True)\n",
    "    ])\n",
    "    upsert_df = spark.createDataFrame(upsert_data, upsert_schema)\n",
    "    upsert_df = upsert_df.withColumn(\"uuid\", lit(None).cast(StringType())) # We'll let Hudi determine the uuid for new records\n",
    "    \n",
    "    # Set the table type to Copy-on-Write for the upsert\n",
    "    hudi_options_upsert = hudi_options_base.copy()\n",
    "    hudi_options_upsert['hoodie.datasource.write.table.type'] = 'COPY_ON_WRITE'\n",
    "    hudi_options_upsert['hoodie.datasource.write.operation'] = 'upsert'\n",
    "    print(\"Writing upserted data to Hudi table...\")\n",
    "    upsert_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_upsert) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table upserted successfully.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "\n",
    "# Call the upsert update function\n",
    "upsert_hudi(spark, hudi_options_base, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba8e3a",
   "metadata": {},
   "source": [
    "Now, that the initial bulk load is done using Copy On Write storage, lets get to the heart of the Apache Hudi. \n",
    "The distinction between snapshot and read-optimized queries is crucial, especially when deciding how to access the upsert data.\n",
    "\n",
    "Snapshot Query\n",
    "A snapshot query provides the most up-to-date view of a Hudi table. It reads the latest committed state of the data, ensuring you get a complete and accurate \"snapshot\" of the table at a specific point in time.\n",
    "\n",
    "How it works (for a Copy on Write table): With a Copy on Write (CoW) table, every update or insert operation creates a new version of the Parquet base file. A snapshot query simply reads all the latest versions of these Parquet files. There are no incremental files to merge, so the query is straightforward and efficient.\n",
    "\n",
    "When to choose it: This is the default and recommended choice for most use cases with CoW tables. It's the simplest way to read the current state of your data. The code spark.read.format(\"hudi\").load(HUDI_BASE_PATH) performs a snapshot query by default.\n",
    "\n",
    "Read-Optimized Query\n",
    "A read-optimized query is designed to be as fast as possible by reading only the Parquet base files, completely ignoring any recent incremental data.\n",
    "\n",
    "How it works (for a Copy on Write table): Since a CoW table already stores all its data in Parquet base files, a read-optimized query behaves exactly the same as a snapshot query. It reads the same set of files and provides the identical result. You will not see a performance difference between the two for a CoW table.\n",
    "\n",
    "When to choose it: This query type is primarily useful for Merge on Read (MoR) tables, not Copy on Write. With a MoR table, a read-optimized query would be faster than a snapshot query because it skips the step of merging the Parquet base files with the Avro/ORC delta logs.  However, this speed comes at the cost of having potentially stale data, as it won't include recent updates that are only in the delta logs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will do a snapshot query on the Hudi table to show the latest state of the data\n",
    "# Every commit in a CoW table implicitly compacts the data into the columnar base file format. \n",
    "# This ensures that the data being read is always in its most optimized form for analytical queries, \n",
    "# without requiring any on-the-fly merging or processing. \n",
    "# This makes snapshot queries on CoW tables very efficient, delivering high performance for analytical workloads where data changes are less frequent.\n",
    "def snapshot_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing snapshot query on Hudi table...\")\n",
    "    trip_snapshot_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    print(\"Snapshot DataFrame loaded from Hudi table:\")\n",
    "    trip_snapshot_df.show(5)\n",
    "    print(\"Creating temporary view for SQL queries...\")\n",
    "    trip_snapshot_df.createOrReplaceTempView(\"hudi_trips_snapshot\")\n",
    "    print(\"Snapshot query result:\")\n",
    "    spark.sql(\"SELECT * FROM hudi_trips_snapshot ORDER BY ts DESC\").show(10)\n",
    "    spark.sql(\"SELECT city, COUNT(*) as trip_count FROM hudi_trips_snapshot GROUP BY city ORDER BY trip_count DESC\").show()\n",
    "    # Saving this to a df for further use if needed\n",
    "    hudi_snapshot_df = spark.sql(\"SELECT * FROM hudi_trips_snapshot ORDER BY ts DESC\")\n",
    "    hudi_snapshot_df.show(5)\n",
    "    return trip_snapshot_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the snapshot query function\n",
    "snapshot_query_hudi(spark, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f988b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funtion will do a read-optimized query on the Hudi table to show the latest state of the data.\n",
    "# The output is exactly the same as the default spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "def read_optimized_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing read-optimized query on Hudi table...\")\n",
    "    trip_ro_df = spark.read.format(\"hudi\").option(\"hoodie.datasource.query.type\", \"read_optimized\").load(HUDI_BASE_PATH)\n",
    "    print(\"Read-Optimized DataFrame loaded from Hudi table:\")\n",
    "    trip_ro_df.show(5)\n",
    "    print(\"Creating temporary view for SQL queries...\")\n",
    "    trip_ro_df.createOrReplaceTempView(\"hudi_trips_ro\")\n",
    "    print(\"Read-Optimized query result:\")\n",
    "    spark.sql(\"SELECT * FROM hudi_trips_ro ORDER BY ts DESC\").show(10)\n",
    "    spark.sql(\"SELECT city, COUNT(*) as trip_count FROM hudi_trips_ro GROUP BY city ORDER BY trip_count DESC\").show()\n",
    "    # Saving this to a df for further use if needed\n",
    "    hudi_ro_df = spark.sql(\"SELECT * FROM hudi_trips_ro ORDER BY ts DESC\")\n",
    "    hudi_ro_df.show(5)\n",
    "    return trip_ro_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the read optimized function\n",
    "read_optimized_query_hudi(spark,hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get all the commits and select the earliest one\n",
    "def read_earliest_commit_incremental(spark, base_path):\n",
    "    \"\"\"\n",
    "    Performs an incremental query from the earliest commit time.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): The active Spark session.\n",
    "        base_path (str): The base path to the Hudi table.\n",
    "    \"\"\"\n",
    "    print(\"Performing incremental query from the earliest commit...\")\n",
    "    \n",
    "    # Read all commits and find the earliest one\n",
    "    # Note: This loads metadata, not the full dataset, so it's efficient.\n",
    "    try:\n",
    "        commits = spark.read.format(\"hudi\").load(base_path).select(\"_hoodie_commit_time\").distinct().collect()\n",
    "        if not commits:\n",
    "            print(\"No commits found in the table. Cannot perform incremental query.\")\n",
    "            return\n",
    "        \n",
    "        # Sort commits and get the earliest one\n",
    "        begin_time = sorted([row[\"_hoodie_commit_time\"] for row in commits])[0]\n",
    "        print(f\"Earliest commit found: {begin_time}\")\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"Error reading commits. Make sure the Hudi table exists at: {base_path}\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # Perform the incremental query using the earliest commit time\n",
    "    inc_query_df = spark.read.format(\"hudi\") \\\n",
    "        .option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    "        .option(\"hoodie.datasource.read.begin.instanttime\", begin_time) \\\n",
    "        .load(base_path)\n",
    "\n",
    "    print(\"Incremental query DataFrame loaded.\")\n",
    "    inc_query_df.show(5)\n",
    "    \n",
    "    # Run a sample SQL query on the incremental data\n",
    "    inc_query_df.createOrReplaceTempView(\"hudi_trips_incremental\")\n",
    "    print(\"Incremental query result (fare > 20.0):\")\n",
    "    spark.sql(\"SELECT trip_id, fare, ts FROM hudi_trips_incremental WHERE fare > 20.0\").show(10)\n",
    "    print(\"Incremental query completed.\")\n",
    "    return inc_query_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# call this function\n",
    "read_earliest_commit_incremental(spark, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to hard delete records from Hudi table\n",
    "def hard_delete_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing hard delete on Hudi table...\")\n",
    "    # Create a DataFrame with the record keys to delete\n",
    "    delete_data = [\n",
    "        (\"trip_3\",),  # Assuming trip_3 exists\n",
    "        (\"trip_50\",)  # Assuming trip_50 exists\n",
    "    ]\n",
    "    delete_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False)\n",
    "    ])\n",
    "    delete_df = spark.createDataFrame(delete_data, delete_schema)\n",
    "    \n",
    "    # Set the operation to 'delete'\n",
    "    hudi_options_delete = hudi_options_base.copy()\n",
    "    hudi_options_delete['hoodie.datasource.write.operation'] = 'delete'\n",
    "    \n",
    "    print(\"Writing delete records to Hudi table...\")\n",
    "    delete_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_delete) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table hard delete completed.\")\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the hard delete function\n",
    "hard_delete_hudi(spark, hudi_options_base, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173bbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a DataFrame with schema evolution and upsert to Hudi table\n",
    "# Schema Evolution in Hudi allows you to add or drop columns over time without having to rebuild the entire table.\n",
    "\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "\n",
    "def schema_evolution_upsert_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing schema evolution upsert to Hudi table...\")\n",
    "    # Create a DataFrame with new data that has an additional column 'payment_method'\n",
    "    schema_evolution_data = [\n",
    "        (\"trip_4\", datetime.fromtimestamp(time.time() + 201), \"rider_4\", \"driver_4\", 150.0, \"sf\", \"2025-09-04\", \"credit_card\"),\n",
    "        (\"trip_5\", datetime.fromtimestamp(time.time() + 202), \"rider_5\", \"driver_5\", 80.0, \"nyc\", \"2025-09-05\", \"paypal\")\n",
    "    ]\n",
    "    schema_evolution_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True),\n",
    "        StructField(\"payment_method\", StringType(), True)  # New column added\n",
    "    ])\n",
    "    schema_evolution_df = spark.createDataFrame(schema_evolution_data, schema_evolution_schema)\n",
    "    schema_evolution_df = schema_evolution_df.withColumn(\"uuid\", lit(None).cast(StringType())) # Let Hudi determine the uuid for new records\n",
    "    \n",
    "    # Set the operation to 'upsert' and enable schema evolution\n",
    "    hudi_options_se = hudi_options_base.copy()\n",
    "    hudi_options_se['hoodie.datasource.write.operation'] = 'upsert'\n",
    "    hudi_options_se['hoodie.datasource.write.table.type'] = 'COPY_ON_WRITE'\n",
    "    hudi_options_se['hoodie.datasource.write.schema.evolution.enable'] = 'true'\n",
    "    \n",
    "    print(\"Writing schema-evolved data to Hudi table...\")\n",
    "    schema_evolution_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_se) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table schema evolution upsert completed.\")\n",
    "\n",
    "\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "\n",
    "# Call the schema evolution upsert function\n",
    "schema_evolution_upsert_hudi(spark, hudi_options_base, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df263ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do a new indexed read query on the Hudi table\n",
    "def indexed_read_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing indexed read query on Hudi table...\")\n",
    "    trip_indexed_df = spark.read.format(\"hudi\").option(\"hoodie.datasource.query.type\", \"snapshot\").load(HUDI_BASE_PATH)\n",
    "    print(\"Indexed Read DataFrame loaded from Hudi table:\")\n",
    "    trip_indexed_df.show(5)\n",
    "    print(\"Creating temporary view for SQL queries...\")\n",
    "    trip_indexed_df.createOrReplaceTempView(\"hudi_trips_indexed\")\n",
    "    print(\"Indexed read query result:\")\n",
    "    spark.sql(\"SELECT * FROM hudi_trips_indexed ORDER BY ts DESC\").show(10)\n",
    "    spark.sql(\"SELECT city, COUNT(*) as trip_count FROM hudi_trips_indexed GROUP BY city ORDER BY trip_count DESC\").show()\n",
    "    # Saving this to a df for further use if needed\n",
    "    hudi_indexed_df = spark.sql(\"SELECT * FROM hudi_trips_indexed ORDER BY ts DESC\")\n",
    "    hudi_indexed_df.show(5)\n",
    "    return trip_indexed_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the indexed read query function\n",
    "indexed_read_query_hudi(spark, hudi_base_path)\n",
    "# This function will clean up the Hudi table data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to show time travel query on the Hudi table\n",
    "def time_travel_query_hudi(spark, HUDI_BASE_PATH):\n",
    "    print(\"Performing time travel query on Hudi table...\")\n",
    "    \n",
    "    # Read all commits and find the earliest one\n",
    "    # Note: This loads metadata, not the full dataset, so it's efficient.\n",
    "    try:\n",
    "        commits = spark.read.format(\"hudi\").load(HUDI_BASE_PATH).select(\"_hoodie_commit_time\").distinct().collect()\n",
    "        if not commits:\n",
    "            print(\"No commits found in the table. Cannot perform time travel query.\")\n",
    "            return\n",
    "        \n",
    "        # Sort commits and get the earliest one\n",
    "        commit_times = sorted([row[\"_hoodie_commit_time\"] for row in commits])\n",
    "        print(f\"Available commits: {commit_times}\")\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"Error reading commits. Make sure the Hudi table exists at: {HUDI_BASE_PATH}\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # For demonstration, we'll pick the second commit if available\n",
    "    if len(commit_times) < 2:\n",
    "        print(\"Not enough commits to perform time travel. Need at least 2 commits.\")\n",
    "        return\n",
    "    \n",
    "    travel_time = commit_times[1]\n",
    "    print(f\"Time travel to commit: {travel_time}\")\n",
    "\n",
    "    # Perform the time travel query using the selected commit time\n",
    "    time_travel_df = spark.read.format(\"hudi\") \\\n",
    "        .option(\"as.of.instant\", travel_time) \\\n",
    "        .load(HUDI_BASE_PATH)\n",
    "    print(\"Time travel query DataFrame loaded.\")\n",
    "    time_travel_df.show(5)\n",
    "    # Run a sample SQL query on the time-traveled data\n",
    "    time_travel_df.createOrReplaceTempView(\"hudi_trips_time_travel\")\n",
    "    print(\"Time travel query result:\")\n",
    "    spark.sql(\"SELECT trip_id, fare, ts FROM hudi_trips_time_travel ORDER BY ts DESC\").show(10)\n",
    "    print(\"Time travel query completed.\")\n",
    "    return time_travel_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the time travel query function\n",
    "time_travel_query_hudi(spark, hudi_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create an index using Bloom filter on the Hudi table\n",
    "# This shows an example of using Bloom filter index for upsert operations with the matching schema\n",
    "# So, this function will create a new DataFrame with the same schema as the existing Hudi table\n",
    "# and perform an upsert operation using the Bloom filter index.\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "def create_bloom_index_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Creating Bloom filter index on Hudi table...\")\n",
    "    print(\"Performing upsert with Bloom filter index enabled...\")\n",
    "    # Create a DataFrame with new data for bulk insert\n",
    "    # This data will either update existing records or insert new ones\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH) \n",
    "    print(\"Current Hudi table data before Bloom index upsert:\") \n",
    "    hudi_read_df.show(5)\n",
    "       # Create the upsert data with values for the new columns\n",
    "    upsert_data = [\n",
    "        # Update an existing record (trip_2)\n",
    "        (\"trip_2\", datetime.fromtimestamp(time.time()), \"rider_2_updated\", \"driver_2\", 100.0, \"la\", \"2025-09-03\", \"credit\", \"some-uuid\"),\n",
    "        # Insert a new record (trip_101)\n",
    "        (\"trip_101\", datetime.fromtimestamp(time.time()), \"rider_101\", \"driver_101\", 200.0, \"nyc\", \"2025-09-15\", \"paypal\", \"another-uuid\")\n",
    "    ]\n",
    "    upsert_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        # Add the missing columns from the existing table's schema\n",
    "        StructField(\"trip_date\", StringType(), True),\n",
    "        StructField(\"payment_method\", StringType(), True),\n",
    "        StructField(\"uuid\", StringType(), True)\n",
    "    ])\n",
    "    upsert_df = spark.createDataFrame(upsert_data, upsert_schema)\n",
    "    \n",
    "    # Set the table type to Copy-on-Write and enable Bloom filter index\n",
    "    hudi_options_bloom = hudi_options_base.copy()\n",
    "    hudi_options_bloom['hoodie.datasource.write.table.type'] = 'COPY_ON_WRITE'\n",
    "    hudi_options_bloom['hoodie.datasource.write.operation'] = 'upsert'\n",
    "    hudi_options_bloom['hoodie.index.type'] = 'BLOOM'\n",
    "\n",
    "    print(\"Writing data with Bloom filter index to Hudi table...\")\n",
    "    upsert_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_bloom) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table with Bloom filter index created successfully.\")\n",
    "\n",
    "    hudi_read_df = spark.read.format(\"hudi\").load(HUDI_BASE_PATH)\n",
    "    hudi_read_df.show()\n",
    "    return hudi_read_df\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the Bloom filter index creation function\n",
    "create_bloom_index_hudi(spark, hudi_options_base, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will show using a Bloom filter index for upsert operations with a mismatched schema\n",
    "# In this example, the new DataFrame has a different schema than the existing Hudi table\n",
    "import time, datetime\n",
    "from datetime import datetime\n",
    "def bloom_index_mismatched_schema_hudi(spark, hudi_options_base, HUDI_BASE_PATH):\n",
    "    print(\"Performing upsert with Bloom filter index and mismatched schema on Hudi table...\")\n",
    "    # Create a DataFrame with new data that has a different schema\n",
    "    # For example, it has an additional column 'vehicle_type' and is missing 'payment_method'\n",
    "    mismatched_data = [\n",
    "        (\"trip_6\", datetime.fromtimestamp(time.time() + 301), \"rider_6\", \"driver_6\", 120.0, \"sf\", \"2025-09-06\", \"car\"),\n",
    "        (\"trip_7\", datetime.fromtimestamp(time.time() + 302), \"rider_7\", \"driver_7\", 90.0, \"nyc\", \"2025-09-07\", \"bike\")\n",
    "    ]\n",
    "    mismatched_schema = StructType([\n",
    "        StructField(\"trip_id\", StringType(), False),\n",
    "        StructField(\"ts\", TimestampType(), True),\n",
    "        StructField(\"rider\", StringType(), True),\n",
    "        StructField(\"driver_id\", StringType(), True),\n",
    "        StructField(\"fare\", DoubleType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"trip_date\", StringType(), True),\n",
    "        StructField(\"vehicle_type\", StringType(), True)  # New column added\n",
    "        # Note: 'payment_method' column is missing\n",
    "    ])\n",
    "    mismatched_df = spark.createDataFrame(mismatched_data, mismatched_schema)\n",
    "    mismatched_df = mismatched_df.withColumn(\"uuid\", lit(None).cast(StringType())) # Let Hudi determine the uuid for new records\n",
    "    \n",
    "    # Set the operation to 'upsert' and enable Bloom filter index\n",
    "    hudi_options_bloom_mismatch = hudi_options_base.copy()\n",
    "    hudi_options_bloom_mismatch['hoodie.datasource.write.operation'] = 'upsert'\n",
    "    hudi_options_bloom_mismatch['hoodie.datasource.write.table.type'] = 'COPY_ON_WRITE'\n",
    "    hudi_options_bloom_mismatch['hoodie.index.type'] = 'BLOOM'\n",
    "    hudi_options_bloom_mismatch['hoodie.datasource.write.schema.evolution.enable'] = 'true'  # Enable schema evolution\n",
    "    \n",
    "    print(\"Writing data with Bloom filter index and mismatched schema to Hudi table...\")\n",
    "    mismatched_df.write.format(\"hudi\") \\\n",
    "        .options(**hudi_options_bloom_mismatch) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    print(\"Hudi table with Bloom filter index and mismatched schema upsert completed.\")\n",
    "\n",
    "# get or create Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark is None:\n",
    "    print(\"No active SparkSession found. Please run your setup cell first.\")\n",
    "    spark=create_spark_session()\n",
    "# Get Hudi parameters and the base path\n",
    "hudi_options_base, hudi_base_path = get_hudi_param()\n",
    "# Call the Bloom filter index creation function\n",
    "bloom_index_mismatched_schema_hudi(spark, hudi_options_base, hudi_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will stop and clean up the Spark session\n",
    "# --- IGNORE ---\n",
    "def stop_spark_session(spark):\n",
    "    if spark:\n",
    "        print(\"Stopping Spark session...\")\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "    else:\n",
    "        print(\"No Spark session to stop.\")\n",
    "# --- IGNORE ---\n",
    "# Call the stop function at the end of your notebook or script\n",
    "# --- IGNORE ---\n",
    "\n",
    "stop_spark_session(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
